use crate::config::Config;
use crate::llm::base_client::{
    LLMClient, LLMError, LLMMessage, LLMResponse, MessageRole, ToolCall as LLMToolCall,
};
use crate::llm::{AnthropicClient, OpenAIClient};
use crate::tools::{AgentToolResult, ToolExecutor, ToolRegistry};
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::Instant;
use thiserror::Error;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

/// Errors that can occur during agent operations.
#[derive(Error, Debug)]
pub enum AgentError {
    /// An error originating from the LLM client (e.g., API error, network issue).
    #[error("LLM interaction failed: {0}")]
    LLMError(#[from] LLMError),
    /// An error originating from a tool execution.
    #[error("Tool execution failed: {0}")]
    ToolError(#[from] crate::tools::ToolError),
    /// An error related to agent configuration.
    #[error("Configuration error: {0}")]
    ConfigError(String),
    /// Agent reached the maximum configured execution steps without completing the task.
    #[error("Agent reached maximum steps ({0}) without completion")]
    MaxStepsReached(u32),
    /// An error occurred during the setup of a new task.
    #[error("Task setup failed: {0}")]
    TaskSetupFailed(String),
    /// An internal logic error within the agent.
    #[allow(dead_code)] // For future, more complex error scenarios
    #[error("Agent logic error: {0}")]
    LogicError(String),
    /// Indicates that the LLM client required for an operation was not available or configured.
    #[allow(dead_code)] // Currently try_new handles this, but could be a distinct error
    #[error("No LLM client available")]
    NoLLMClient,
}

/// Represents the various states an agent can be in during its execution loop.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)] // Added PartialEq
pub enum AgentState {
    /// The agent is initializing or setting up for a task.
    Initializing,
    /// The agent is currently "thinking" (i.e., waiting for a response from the LLM).
    Thinking,
    /// The agent is in the process of calling a tool.
    CallingTool,
    /// The agent is processing the results received from a tool execution.
    ProcessingToolResult,
    /// The agent is reflecting on past actions or results.
    /// TODO: Actual reflection logic is not deeply implemented; this state is a placeholder.
    Reflecting,
    /// The agent has successfully completed its task.
    Completed,
    /// The agent has failed to complete its task or encountered an unrecoverable error.
    Failed,
}

/// Represents a single step in the agent's execution of a task.
/// This struct is used for logging and trajectory recording.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentStep {
    /// The sequential number of this step in the execution.
    pub step_number: u32,
    /// The state of the agent at the end of this step.
    pub state: AgentState,
    /// Messages sent to the LLM at the beginning of this step.
    pub messages_to_llm: Option<Vec<LLMMessage>>,
    /// The raw response received from the LLM during this step.
    pub llm_response: Option<LLMResponse>,
    /// Any tool calls requested by the LLM in this step.
    pub tool_calls_made: Option<Vec<LLMToolCall>>,
    /// The results obtained from executing the requested tools.
    pub tool_results: Option<Vec<AgentToolResult>>,
    /// Optional reflection text generated by the agent about this step.
    // TODO: Implement reflection logic if needed, similar to Python's `reflect_on_result`.
    pub reflection: Option<String>,
    /// Optional error message if an error occurred during this step.
    pub error: Option<String>,
    /// Duration of this step in milliseconds.
    pub duration_ms: u128,
}

/// Records the entire execution trajectory of an agent for a given task.
/// This includes all steps taken, final outcome, and metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentExecution {
    /// The description of the task the agent was asked to perform.
    pub task: String,
    /// Unix timestamp (seconds since epoch) indicating when the task execution started.
    pub start_time: u64,
    /// Optional Unix timestamp (seconds since epoch) for when the task execution ended.
    pub end_time: Option<u64>,
    /// A list of all steps taken by the agent during this execution.
    pub steps: Vec<AgentStep>,
    /// The final result or summary message from the agent upon completion or failure.
    pub final_result: Option<String>,
    /// Boolean indicating whether the task was successfully completed.
    pub success: bool,
    /// Optional total number of LLM tokens used during this execution.
    // TODO: Aggregate token usage from llm_response.usage if available.
    pub total_tokens_used: Option<u32>,
    /// Optional error message if the agent execution failed overall.
    pub error_message: Option<String>,
}

/// Represents events that can occur during an agent's task execution.
/// These can be used for real-time updates to a CLI or UI, or for detailed logging.
/// TODO: The data carried by these events (e.g., `u32` for step numbers) might be simplified
/// if detailed event consumption isn't immediately planned. For now, they mirror Python's intent.
#[derive(Debug)]
#[allow(dead_code)] // Variants and fields are for future detailed CLI/UI, not all consumed yet
pub enum AgentEvent {
    /// Signals the beginning of a new agent step. Contains step number.
    StepBegin(u32),
    /// Signals a change in the agent's state within a step. Contains step number and new state.
    StepStateChange(u32, AgentState),
    /// Signals that a request has been sent to the LLM. Contains step number and messages sent.
    LLMRequestSent(u32, Vec<LLMMessage>),
    /// Signals that a response has been received from the LLM. Contains step number and the response.
    LLMResponseReceived(u32, Box<LLMResponse>),
    /// Signals an attempt to call a tool. Contains step number and the tool call request.
    ToolCallAttempt(u32, Box<LLMToolCall>),
    /// Signals the result of a tool call. Contains step number and the tool result.
    ToolCallResult(u32, Box<AgentToolResult>),
    /// Signals that the agent has completed its task. Contains the full execution trajectory.
    TaskCompleted(Box<AgentExecution>),
    /// Signals that the agent has failed its task. Contains the full execution trajectory.
    TaskFailed(Box<AgentExecution>),
    /// A general status update message from the agent.
    StatusUpdate(String),
}

/// Defines the core capabilities of an agent.
#[async_trait]
pub trait Agent: Send + Sync {
    /// Returns the name of the agent implementation (e.g., "TraeAgentRs").
    fn get_name(&self) -> String;

    /// Initializes a new task for the agent, preparing it for execution.
    /// This typically involves setting the task description, initializing conversation history,
    /// and processing task-specific arguments.
    ///
    /// # Arguments
    /// * `task`: A string describing the task to be performed.
    /// * `task_args`: Optional `serde_json::Value` containing additional arguments or context
    ///                for the task (e.g., project path, `must_patch` flag).
    async fn new_task(
        &mut self,
        task: String,
        task_args: Option<serde_json::Value>,
    ) -> Result<(), AgentError>;

    /// Executes the currently configured task.
    /// This involves iteratively interacting with the LLM, calling tools, and managing state
    /// until the task is completed, fails, or reaches maximum steps.
    ///
    /// # Arguments
    /// * `event_sender`: An optional `mpsc::Sender<AgentEvent>` to send real-time updates
    ///                   about the agent's progress. If `None`, events are not sent.
    ///
    /// # Returns
    /// A `Result` containing an `AgentExecution` summary upon completion (successful or not),
    /// or an `AgentError` if a critical error prevents execution from starting or finishing properly.
    async fn execute_task(
        &mut self,
        event_sender: Option<mpsc::Sender<AgentEvent>>,
    ) -> Result<AgentExecution, AgentError>;

    /// Executes a single turn in an interactive conversation.
    /// This method assumes `new_task` has been called with the user's latest input,
    /// which sets up `self.current_task` and potentially appends to `self.conversation_history`.
    /// It then runs the agent's core logic for one cycle (LLM call, optional tool calls, final LLM response for the turn).
    ///
    /// # Arguments
    /// * `event_sender`: An optional `mpsc::Sender<AgentEvent>` for real-time updates.
    ///
    /// # Returns
    /// A `Result` containing a `Vec<LLMMessage>` which includes all messages generated by the
    /// agent during this turn (e.g., assistant's thoughts, tool calls, tool results, and final assistant response).
    /// The caller is responsible for managing the overall conversation history by appending these messages.
    /// Returns an `AgentError` if a critical error occurs during the turn.
    async fn execute_interactive_turn(
        &mut self,
        event_sender: Option<mpsc::Sender<AgentEvent>>,
    ) -> Result<Vec<LLMMessage>, AgentError>;
}

/// Holds the common state and shared logic for an agent.
///
/// Specific agent implementations (like `TraeAgent`) will typically compose `BaseAgent`
/// to manage configuration, LLM interaction, tool execution, conversation history,
/// and other common aspects of an agent's lifecycle.
pub struct BaseAgent {
    /// The name of this agent instance (primarily for logging/identification).
    #[allow(dead_code)]
    pub name: String,
    /// Shared application configuration.
    #[allow(dead_code)]
    // Config is used in try_new to init other fields, but this Arc copy isn't read later by BaseAgent methods.
    pub config: Arc<Config>,
    /// The LLM client instance used for interacting with the language model.
    pub llm_client: Arc<dyn LLMClient>,
    /// A registry of available tools that the agent can use.
    pub tool_registry: Arc<ToolRegistry>,
    /// An executor responsible for running tools requested by the LLM.
    pub tool_executor: ToolExecutor,
    /// The description of the current task being processed by the agent.
    pub current_task: Option<String>,
    /// The history of messages exchanged with the LLM for the current task.
    pub conversation_history: Vec<LLMMessage>,
    /// The maximum number of steps the agent is allowed to take for a task.
    pub max_steps: u32,
    /// The file system path to the project or working directory for the current task.
    pub project_path: Option<String>,
    /// A flag indicating whether the agent is required to produce a patch for the task.
    pub must_patch: bool,
    /// Optional base commit hash for diffing.
    pub base_commit: Option<String>,
}

impl BaseAgent {
    /// Attempts to create a new `BaseAgent`.
    ///
    /// Initializes the LLM client based on the provided configuration and sets up
    /// the tool executor with tools from the given registry.
    ///
    /// # Arguments
    /// * `config`: Shared application configuration.
    /// * `tool_registry`: Shared registry of available tools.
    ///
    /// # Returns
    /// A `Result` containing the new `BaseAgent` or an `AgentError` if initialization fails
    /// (e.g., unsupported LLM provider, client creation error).
    pub async fn try_new(
        config: Arc<Config>,
        tool_registry: Arc<ToolRegistry>,
    ) -> Result<Self, AgentError> {
        let provider_name = &config.default_provider;
        let provider_config = config
            .get_current_provider_config()
            .map_err(|e| AgentError::ConfigError(e.to_string()))?;

        let llm_client: Arc<dyn LLMClient> = match provider_name.as_str() {
            "openai" => Arc::new(
                OpenAIClient::new(
                    provider_config.api_key.clone(),
                    None,
                    provider_config.clone(),
                )
                .await?,
            ),
            "anthropic" => Arc::new(
                AnthropicClient::new(
                    provider_config.api_key.clone(),
                    None,
                    provider_config.clone(),
                )
                .await?,
            ),
            _ => {
                return Err(AgentError::ConfigError(format!(
                    "Unsupported LLM provider: {}",
                    provider_name
                )))
            }
        };

        let all_tools_from_registry: Vec<Arc<dyn crate::tools::Tool + Send + Sync>> =
            tool_registry.get_all_tools_arc();
        let tool_executor = ToolExecutor::new(all_tools_from_registry);

        Ok(Self {
            name: "BaseAgent".to_string(), // Specific agents can override this after creation if needed
            config: config.clone(),
            llm_client,
            tool_registry,
            tool_executor,
            current_task: None,
            conversation_history: Vec::new(),
            max_steps: config.max_steps,
            project_path: config.working_dir.clone(),
            must_patch: false,
            base_commit: None, // Initialize as None
        })
    }
}

/// The core execution loop for an agent.
///
/// This function is called by a concrete `Agent` implementation (like `TraeAgent`)
/// to drive its task execution. It manages the step-by-step interaction with the LLM,
/// tool execution, state updates, and conversation history.
///
/// # Type Parameters
/// * `FShouldStop`: A closure type that determines if the agent should stop execution based on the LLM response and current state.
/// * `FProcessCompletion`: A closure type that processes the final LLM response upon successful task completion.
///
/// # Arguments
/// * `base_agent`: A mutable reference to the `BaseAgent` containing shared state.
/// * `initial_messages`: The initial set of messages (e.g., system prompt, user task) to start the conversation.
/// * `event_sender`: Optional sender for `AgentEvent`s to report progress.
/// * `should_stop_fn`: Closure that takes `(&LLMResponse, current_step, max_steps)` and returns `(bool, Option<String>)`.
///   The boolean indicates if execution should stop. The `Option<String>` provides an error message
///   for the LLM if stopping is due to a validation failure (e.g., empty patch).
/// * `process_completion_fn`: Closure that takes `&LLMResponse` and returns `Option<String>` for the final result message.
///
/// # Returns
/// An `AgentExecution` struct summarizing the entire task run.
pub async fn common_execute_task_loop<FShouldStop, FProcessCompletion>(
    base_agent: &mut BaseAgent,
    initial_messages: Vec<LLMMessage>,
    event_sender: Option<mpsc::Sender<AgentEvent>>,
    should_stop_fn: &FShouldStop,
    process_completion_fn: &FProcessCompletion,
) -> Result<AgentExecution, AgentError>
where
    FShouldStop: Fn(&LLMResponse, u32, u32) -> (bool, Option<String>),
    FProcessCompletion: Fn(&LLMResponse) -> Option<String>,
{
    let task_name = base_agent
        .current_task
        .clone()
        .unwrap_or_else(|| "Untitled Task".to_string());
    info!(task = %task_name, "Starting task execution");

    let mut execution = AgentExecution {
        task: task_name.clone(),
        start_time: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
        end_time: None,
        steps: Vec::new(),
        final_result: None,
        success: false,
        total_tokens_used: Some(0), // TODO: Ensure this is correctly summed from LLMUsage
        error_message: None,
    };

    base_agent.conversation_history = initial_messages;
    let mut current_step_number = 1;

    while current_step_number <= base_agent.max_steps {
        let step_start_time = Instant::now();
        if let Some(sender) = &event_sender {
            _ = sender
                .send(AgentEvent::StepBegin(current_step_number))
                .await;
            _ = sender
                .send(AgentEvent::StepStateChange(
                    current_step_number,
                    AgentState::Thinking,
                ))
                .await;
        }

        debug!(
            step = current_step_number,
            messages_count = base_agent.conversation_history.len(),
            "Sending request to LLM"
        );
        if let Some(sender) = &event_sender {
            _ = sender
                .send(AgentEvent::LLMRequestSent(
                    current_step_number,
                    base_agent.conversation_history.clone(),
                ))
                .await;
        }

        let tool_definitions = base_agent.tool_registry.get_all_tool_definitions();
        let llm_response_result = base_agent
            .llm_client
            .chat(
                base_agent.conversation_history.clone(),
                if tool_definitions.is_empty() {
                    None
                } else {
                    Some(tool_definitions)
                },
                None,
            )
            .await;

        let mut agent_step = AgentStep {
            step_number: current_step_number,
            state: AgentState::Thinking,
            messages_to_llm: Some(base_agent.conversation_history.clone()),
            llm_response: None,
            tool_calls_made: None,
            tool_results: None,
            reflection: None, // TODO: Implement reflection if needed
            error: None,
            duration_ms: 0,
        };

        match llm_response_result {
            Ok(llm_response) => {
                agent_step.llm_response = Some(llm_response.clone());
                if let Some(sender) = &event_sender {
                    _ = sender
                        .send(AgentEvent::LLMResponseReceived(
                            current_step_number,
                            Box::new(llm_response.clone()),
                        ))
                        .await;
                }
                if let Some(usage) = &llm_response.usage {
                    execution.total_tokens_used = execution
                        .total_tokens_used
                        .map_or(Some(usage.total_tokens), |current| {
                            Some(current + usage.total_tokens)
                        });
                }

                base_agent
                    .conversation_history
                    .push(llm_response.choices[0].message.clone());

                let (stop_signal, error_msg_for_llm) =
                    should_stop_fn(&llm_response, current_step_number, base_agent.max_steps);

                if stop_signal {
                    agent_step.state = AgentState::Completed;
                    execution.success = true;
                    execution.final_result = process_completion_fn(&llm_response);
                    info!(task = %task_name, "Task marked completed by agent logic.");
                    agent_step.duration_ms = step_start_time.elapsed().as_millis();
                    execution.steps.push(agent_step);
                    break;
                } else if let Some(llm_err_msg) = error_msg_for_llm {
                    // error_msg_for_llm is moved here
                    warn!(task = %task_name, "Task completion validation failed: {}", llm_err_msg);
                    base_agent.conversation_history.push(LLMMessage {
                        role: MessageRole::User,
                        content: Some(llm_err_msg), // llm_err_msg is used
                        name: None,
                        tool_calls: None,
                        tool_call_id: None,
                    });
                    agent_step.state = AgentState::Thinking;
                }

                // If not stopping, check for tool calls (only if no validation error message was generated,
                // as that message needs to go to LLM first).
                // The error_msg_for_llm was moved in the `if let Some` above.
                // We need to check its original value or restructure.
                // Let's check if agent_step.state is still Thinking (i.e., didn't get a validation error message)
                if agent_step.state == AgentState::Thinking {
                    // Check if state was changed by validation error
                    if let Some(tool_calls) = llm_response.choices[0].message.tool_calls.clone() {
                        if !tool_calls.is_empty() {
                            agent_step.state = AgentState::CallingTool;
                            agent_step.tool_calls_made = Some(tool_calls.clone());
                            if let Some(sender) = &event_sender {
                                _ = sender
                                    .send(AgentEvent::StepStateChange(
                                        current_step_number,
                                        AgentState::CallingTool,
                                    ))
                                    .await;
                                for tc_req in &tool_calls {
                                    _ = sender
                                        .send(AgentEvent::ToolCallAttempt(
                                            current_step_number,
                                            Box::new(tc_req.clone()),
                                        ))
                                        .await;
                                }
                            }

                            let executed_tool_results = base_agent
                                .tool_executor
                                .sequential_tool_calls(&tool_calls)
                                .await;
                            agent_step.tool_results = Some(executed_tool_results.clone());
                            if let Some(sender) = &event_sender {
                                for tres in &executed_tool_results {
                                    _ = sender
                                        .send(AgentEvent::ToolCallResult(
                                            current_step_number,
                                            Box::new(tres.clone()),
                                        ))
                                        .await;
                                }
                            }

                            for tool_result in executed_tool_results {
                                let tool_name_for_message = tool_calls
                                    .iter()
                                    .find(|tc| tc.id == tool_result.tool_call_id)
                                    .map(|tc| tc.function.name.clone());
                                base_agent.conversation_history.push(LLMMessage {
                                    tool_call_id: Some(tool_result.tool_call_id),
                                    role: MessageRole::Tool,
                                    name: tool_name_for_message,
                                    content: Some(tool_result.result.unwrap_or_else(|| {
                                        tool_result.error.unwrap_or_else(|| {
                                            "Error executing tool, no details.".to_string()
                                        })
                                    })),
                                    tool_calls: None,
                                });
                            }
                            agent_step.state = AgentState::ProcessingToolResult;
                        }
                    } else {
                        debug!(
                            step = current_step_number,
                            "LLM provided text response, no tool calls."
                        );
                    }
                }
            }
            Err(e) => {
                error!(step = current_step_number, error = %e, "LLM chat failed");
                agent_step.state = AgentState::Failed;
                agent_step.error = Some(e.to_string());
                execution.error_message = Some(e.to_string());
                execution.success = false;
                agent_step.duration_ms = step_start_time.elapsed().as_millis();
                execution.steps.push(agent_step);
                break;
            }
        }
        agent_step.duration_ms = step_start_time.elapsed().as_millis();
        execution.steps.push(agent_step);
        current_step_number += 1;
    }

    if !execution.success && execution.error_message.is_none() {
        warn!(task = %task_name, steps = current_step_number -1, "Task reached max steps or ended without explicit completion/failure.");
        if current_step_number > base_agent.max_steps {
            execution.error_message =
                Some(AgentError::MaxStepsReached(base_agent.max_steps).to_string());
        } else {
            execution.error_message =
                Some("Task ended without explicit completion signal.".to_string());
        }
    }

    execution.end_time = Some(
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
    );

    if let Some(sender) = event_sender {
        if execution.success {
            _ = sender
                .send(AgentEvent::TaskCompleted(Box::new(execution.clone())))
                .await;
        } else {
            _ = sender
                .send(AgentEvent::TaskFailed(Box::new(execution.clone())))
                .await;
        }
    }
    info!(task = %task_name, success = execution.success, "Task execution finished.");
    Ok(execution)
}
