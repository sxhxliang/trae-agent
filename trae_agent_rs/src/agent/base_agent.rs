use crate::config::Config;
use crate::llm::base_client::{
    LLMClient, LLMError, LLMMessage, LLMResponse, MessageRole, ToolCall as LLMToolCall, LLMUsage,
};
use crate::llm::{AnthropicClient, OpenAIClient};
use crate::tools::{AgentToolResult, ToolExecutor, ToolRegistry};
use crate::utils::trajectory_recorder::TrajectoryRecorder; // Added
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::Instant;
use thiserror::Error;
use tokio::sync::mpsc;
use tracing::{debug, error, info, warn};

/// Errors that can occur during agent operations.
#[derive(Error, Debug)]
pub enum AgentError {
    /// An error originating from the LLM client (e.g., API error, network issue).
    #[error("LLM interaction failed: {0}")]
    LLMError(#[from] LLMError),
    /// An error originating from a tool execution.
    #[error("Tool execution failed: {0}")]
    ToolError(#[from] crate::tools::ToolError),
    /// An error related to agent configuration.
    #[error("Configuration error: {0}")]
    ConfigError(String),
    /// Agent reached the maximum configured execution steps without completing the task.
    #[error("Agent reached maximum steps ({0}) without completion")]
    MaxStepsReached(u32),
    /// An error occurred during the setup of a new task.
    #[error("Task setup failed: {0}")]
    TaskSetupFailed(String),
    /// An internal logic error within the agent.
    #[allow(dead_code)] // For future, more complex error scenarios
    #[error("Agent logic error: {0}")]
    LogicError(String),
    /// Indicates that the LLM client required for an operation was not available or configured.
    #[allow(dead_code)] // Currently try_new handles this, but could be a distinct error
    #[error("No LLM client available")]
    NoLLMClient,
}

/// Represents the various states an agent can be in during its execution loop.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)] // Added PartialEq
pub enum AgentState {
    /// The agent is initializing or setting up for a task.
    Initializing,
    /// The agent is currently "thinking" (i.e., waiting for a response from the LLM).
    Thinking,
    /// The agent is in the process of calling a tool.
    CallingTool,
    /// The agent is processing the results received from a tool execution.
    ProcessingToolResult,
    /// The agent is reflecting on past actions or results.
    /// TODO: Actual reflection logic is not deeply implemented; this state is a placeholder.
    Reflecting,
    /// The agent has successfully completed its task.
    Completed,
    /// The agent has failed to complete its task or encountered an unrecoverable error.
    Failed,
}

/// Represents a single step in the agent's execution of a task.
/// This struct is used for logging and trajectory recording.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentStep {
    /// The sequential number of this step in the execution.
    pub step_number: u32,
    /// The state of the agent at the end of this step.
    pub state: AgentState,
    /// Messages sent to the LLM at the beginning of this step.
    pub messages_to_llm: Option<Vec<LLMMessage>>,
    /// The raw response received from the LLM during this step.
    pub llm_response: Option<LLMResponse>,
    /// Any tool calls requested by the LLM in this step.
    pub tool_calls_made: Option<Vec<LLMToolCall>>,
    /// The results obtained from executing the requested tools.
    pub tool_results: Option<Vec<AgentToolResult>>,
    /// Optional reflection text generated by the agent about this step.
    // TODO: Implement reflection logic if needed, similar to Python's `reflect_on_result`.
    pub reflection: Option<String>,
    /// Optional error message if an error occurred during this step.
    pub error: Option<String>,
    /// Duration of this step in milliseconds.
    pub duration_ms: u128,
}

/// Records the entire execution trajectory of an agent for a given task.
/// This includes all steps taken, final outcome, and metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentExecution {
    /// The description of the task the agent was asked to perform.
    pub task: String,
    /// Unix timestamp (seconds since epoch) indicating when the task execution started.
    pub start_time: u64,
    /// Optional Unix timestamp (seconds since epoch) for when the task execution ended.
    pub end_time: Option<u64>,
    /// A list of all steps taken by the agent during this execution.
    pub steps: Vec<AgentStep>,
    /// The final result or summary message from the agent upon completion or failure.
    pub final_result: Option<String>,
    /// Boolean indicating whether the task was successfully completed.
    pub success: bool,
    /// Optional total number of LLM tokens used during this execution, as LLMUsage.
    pub total_tokens_used: Option<LLMUsage>, // Changed from Option<u32>
    /// Optional error message if the agent execution failed overall.
    pub error_message: Option<String>,
}

/// Represents events that can occur during an agent's task execution.
/// These can be used for real-time updates to a CLI or UI, or for detailed logging.
/// TODO: The data carried by these events (e.g., `u32` for step numbers) might be simplified
/// if detailed event consumption isn't immediately planned. For now, they mirror Python's intent.
#[derive(Debug)]
#[allow(dead_code)] // Variants and fields are for future detailed CLI/UI, not all consumed yet
pub enum AgentEvent {
    /// Signals the beginning of a new agent step. Contains step number.
    StepBegin(u32),
    /// Signals a change in the agent's state within a step. Contains step number and new state.
    StepStateChange(u32, AgentState),
    /// Signals that a request has been sent to the LLM. Contains step number and messages sent.
    LLMRequestSent(u32, Vec<LLMMessage>),
    /// Signals that a response has been received from the LLM. Contains step number and the response.
    LLMResponseReceived(u32, Box<LLMResponse>),
    /// Signals an attempt to call a tool. Contains step number and the tool call request.
    ToolCallAttempt(u32, Box<LLMToolCall>),
    /// Signals the result of a tool call. Contains step number and the tool result.
    ToolCallResult(u32, Box<AgentToolResult>),
    /// Signals that the agent has completed its task. Contains the full execution trajectory.
    TaskCompleted(Box<AgentExecution>),
    /// Signals that the agent has failed its task. Contains the full execution trajectory.
    TaskFailed(Box<AgentExecution>),
    /// A general status update message from the agent.
    StatusUpdate(String),
}

/// Defines the core capabilities of an agent.
#[async_trait]
pub trait Agent: Send + Sync {
    /// Returns the name of the agent implementation (e.g., "TraeAgentRs").
    fn get_name(&self) -> String;

    /// Initializes a new task for the agent, preparing it for execution.
    /// This typically involves setting the task description, initializing conversation history,
    /// and processing task-specific arguments.
    ///
    /// # Arguments
    /// * `task`: A string describing the task to be performed.
    /// * `task_args`: Optional `serde_json::Value` containing additional arguments or context
    ///                for the task (e.g., project path, `must_patch` flag).
    async fn new_task(
        &mut self,
        task: String,
        task_args: Option<serde_json::Value>,
    ) -> Result<(), AgentError>;

    /// Executes the currently configured task.
    /// This involves iteratively interacting with the LLM, calling tools, and managing state
    /// until the task is completed, fails, or reaches maximum steps.
    ///
    /// # Arguments
    /// * `event_sender`: An optional `mpsc::Sender<AgentEvent>` to send real-time updates
    ///                   about the agent's progress. If `None`, events are not sent.
    ///
    /// # Returns
    /// A `Result` containing an `AgentExecution` summary upon completion (successful or not),
    /// or an `AgentError` if a critical error prevents execution from starting or finishing properly.
    async fn execute_task(
        &mut self,
        event_sender: Option<mpsc::Sender<AgentEvent>>,
    ) -> Result<AgentExecution, AgentError>;

    /// Executes a single turn in an interactive conversation.
    /// This method assumes `new_task` has been called with the user's latest input,
    /// which sets up `self.current_task` and potentially appends to `self.conversation_history`.
    /// It then runs the agent's core logic for one cycle (LLM call, optional tool calls, final LLM response for the turn).
    ///
    /// # Arguments
    /// * `event_sender`: An optional `mpsc::Sender<AgentEvent>` for real-time updates.
    ///
    /// # Returns
    /// A `Result` containing a `Vec<LLMMessage>` which includes all messages generated by the
    /// agent during this turn (e.g., assistant's thoughts, tool calls, tool results, and final assistant response).
    /// The caller is responsible for managing the overall conversation history by appending these messages.
    /// Returns an `AgentError` if a critical error occurs during the turn.
    async fn execute_interactive_turn(
        &mut self,
        event_sender: Option<mpsc::Sender<AgentEvent>>,
    ) -> Result<Vec<LLMMessage>, AgentError>;
}

/// Holds the common state and shared logic for an agent.
///
/// Specific agent implementations (like `TraeAgent`) will typically compose `BaseAgent`
/// to manage configuration, LLM interaction, tool execution, conversation history,
/// and other common aspects of an agent's lifecycle.
pub struct BaseAgent {
    /// The name of this agent instance (primarily for logging/identification).
    #[allow(dead_code)]
    pub name: String,
    /// Shared application configuration.
    #[allow(dead_code)]
    // Config is used in try_new to init other fields, but this Arc copy isn't read later by BaseAgent methods.
    pub config: Arc<Config>,
    /// The LLM client instance used for interacting with the language model.
    pub llm_client: Arc<dyn LLMClient>,
    /// A registry of available tools that the agent can use.
    pub tool_registry: Arc<ToolRegistry>,
    /// An executor responsible for running tools requested by the LLM.
    pub tool_executor: ToolExecutor,
    /// The description of the current task being processed by the agent.
    pub current_task: Option<String>,
    /// The history of messages exchanged with the LLM for the current task.
    pub conversation_history: Vec<LLMMessage>,
    /// The maximum number of steps the agent is allowed to take for a task.
    pub max_steps: u32,
    /// The file system path to the project or working directory for the current task.
    pub project_path: Option<String>,
    /// A flag indicating whether the agent is required to produce a patch for the task.
    pub must_patch: bool,
    /// Optional base commit hash for diffing.
    pub base_commit: Option<String>,
    /// Optional path to save the generated patch file.
    pub patch_path: Option<String>, // Added for patch saving
    /// Optional trajectory recorder.
    pub trajectory_recorder: Option<TrajectoryRecorder>, // Added
}

impl BaseAgent {
    /// Attempts to create a new `BaseAgent`.
    ///
    /// Initializes the LLM client based on the provided configuration and sets up
    /// the tool executor with tools from the given registry.
    ///
    /// # Arguments
    /// * `config`: Shared application configuration.
    /// * `tool_registry`: Shared registry of available tools.
    ///
    /// # Returns
    /// A `Result` containing the new `BaseAgent` or an `AgentError` if initialization fails
    /// (e.g., unsupported LLM provider, client creation error).
    pub async fn try_new(
        config: Arc<Config>,
        tool_registry: Arc<ToolRegistry>,
    ) -> Result<Self, AgentError> {
        let provider_name = &config.default_provider;
        let provider_config = config
            .get_current_provider_config()
            .map_err(|e| AgentError::ConfigError(e.to_string()))?;

        let llm_client: Arc<dyn LLMClient> = match provider_name.as_str() {
            "openai" => Arc::new(
                OpenAIClient::new(
                    provider_config.api_key.clone(),
                    None,
                    provider_config.clone(),
                )
                .await?,
            ),
            "anthropic" => Arc::new(
                AnthropicClient::new(
                    provider_config.api_key.clone(),
                    None,
                    provider_config.clone(),
                )
                .await?,
            ),
            _ => {
                return Err(AgentError::ConfigError(format!(
                    "Unsupported LLM provider: {}",
                    provider_name
                )))
            }
        };

        let all_tools_from_registry: Vec<Arc<dyn crate::tools::Tool + Send + Sync>> =
            tool_registry.get_all_tools_arc();
        let tool_executor = ToolExecutor::new(all_tools_from_registry);

        Ok(Self {
            name: "BaseAgent".to_string(), // Specific agents can override this after creation if needed
            config: config.clone(),
            llm_client,
            tool_registry,
            tool_executor,
            current_task: None,
            conversation_history: Vec::new(),
            max_steps: config.max_steps,
            project_path: config.working_dir.clone(),
            must_patch: false,
            base_commit: None,
            patch_path: None, // Initialize as None
            trajectory_recorder: None,
        })
    }

    /// Sets the trajectory recorder for the agent.
    pub fn set_trajectory_recorder(&mut self, recorder: TrajectoryRecorder) {
        self.trajectory_recorder = Some(recorder);
        // If LLMClient also needs it (like in Python), we'd pass it here.
        // self.llm_client.set_trajectory_recorder(self.trajectory_recorder.as_ref().unwrap().clone()); // Requires LLMClient to have this method and recorder to be Clone or Arc
    }
}

/// The core execution loop for an agent.
///
/// This function is called by a concrete `Agent` implementation (like `TraeAgent`)
/// to drive its task execution. It manages the step-by-step interaction with the LLM,
/// tool execution, state updates, and conversation history.
///
// Define StopReason (can be moved to a separate types.rs if preferred)
#[derive(Debug, Clone, PartialEq)]
pub enum StopReason {
    /// Agent should continue execution.
    Continue,
    /// Task successfully completed (e.g., by tool call or textual confirmation) and passed validation.
    TaskCompleted,
    /// Task considered complete by LLM/logic, but failed validation (e.g., empty patch).
    /// Contains a message to be sent back to the LLM.
    ValidationFailed(String),
    /// Maximum execution steps reached.
    MaxStepsReached,
}

/// # Type Parameters
/// * `FShouldStop`: A closure type that determines if the agent should stop execution based on the LLM response and current state.
/// * `FProcessCompletion`: A closure type that processes the final LLM response upon successful task completion.
///
/// # Arguments
/// * `base_agent`: A mutable reference to the `BaseAgent` containing shared state.
/// * `initial_messages`: The initial set of messages (e.g., system prompt, user task) to start the conversation.
/// * `event_sender`: Optional sender for `AgentEvent`s to report progress.
/// * `should_stop_fn`: Closure that takes `(&LLMResponse, current_step, max_steps)` and returns `StopReason`.
/// * `process_completion_fn`: Closure that takes `&LLMResponse` and returns `Option<String>` for the final result message.
///
/// # Returns
/// An `AgentExecution` struct summarizing the entire task run.
pub async fn common_execute_task_loop<FShouldStop, FProcessCompletion>(
    base_agent: &mut BaseAgent,
    initial_messages: Vec<LLMMessage>,
    event_sender: Option<mpsc::Sender<AgentEvent>>,
    should_stop_fn: &FShouldStop,
    process_completion_fn: &FProcessCompletion,
) -> Result<AgentExecution, AgentError>
where
    FShouldStop: Fn(&LLMResponse, u32, u32) -> StopReason,
    FProcessCompletion: Fn(&LLMResponse) -> Option<String>,
{
    let task_name = base_agent
        .current_task
        .clone()
        .unwrap_or_else(|| "Untitled Task".to_string());
    info!(task = %task_name, "Starting task execution");

    let mut execution = AgentExecution {
        task: task_name.clone(),
        start_time: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
        end_time: None,
        steps: Vec::new(),
        final_result: None,
        success: false,
        total_tokens_used: None, // Initialize as None
        error_message: None,
    };

    base_agent.conversation_history = initial_messages;
    let mut current_step_number = 1;

    // Record initial state if trajectory recorder is present
    // This is more like Python's start_recording which happens in TraeAgent::new_task
    // So, actual start_recording should be called before this loop.

    while current_step_number <= base_agent.max_steps {
        let step_start_time = Instant::now();
        if let Some(sender) = &event_sender {
            _ = sender
                .send(AgentEvent::StepBegin(current_step_number))
                .await;
            _ = sender
                .send(AgentEvent::StepStateChange(
                    current_step_number,
                    AgentState::Thinking,
                ))
                .await;
        }

        debug!(
            step = current_step_number,
            messages_count = base_agent.conversation_history.len(),
            "Sending request to LLM"
        );
        if let Some(sender) = &event_sender {
            _ = sender
                .send(AgentEvent::LLMRequestSent(
                    current_step_number,
                    base_agent.conversation_history.clone(),
                ))
                .await;
        }

        let tool_definitions = base_agent.tool_registry.get_all_tool_definitions();
        let llm_response_result = base_agent
            .llm_client
            .chat(
                base_agent.conversation_history.clone(),
                if tool_definitions.is_empty() {
                    None
                } else {
                    Some(tool_definitions)
                },
                None,
            )
            .await;

        let mut agent_step = AgentStep {
            step_number: current_step_number,
            state: AgentState::Thinking,
            messages_to_llm: Some(base_agent.conversation_history.clone()),
            llm_response: None,
            tool_calls_made: None,
            tool_results: None,
            reflection: None, // TODO: Implement reflection if needed
            error: None,
            duration_ms: 0,
        };

        match llm_response_result {
            Ok(llm_response) => {
                agent_step.llm_response = Some(llm_response.clone());
                if let Some(sender) = &event_sender {
                    _ = sender
                        .send(AgentEvent::LLMResponseReceived(
                            current_step_number,
                            Box::new(llm_response.clone()),
                        ))
                        .await;
                }
                if let Some(new_usage) = &llm_response.usage {
                    execution.total_tokens_used = match execution.total_tokens_used.as_mut() {
                        Some(existing_usage) => {
                            existing_usage.prompt_tokens += new_usage.prompt_tokens;
                            existing_usage.completion_tokens = Some(
                                existing_usage.completion_tokens.unwrap_or(0)
                                    + new_usage.completion_tokens.unwrap_or(0),
                            );
                            existing_usage.total_tokens += new_usage.total_tokens;
                            Some(existing_usage.clone()) // Clone because we borrowed mutably
                        }
                        None => Some(new_usage.clone()),
                    };
                }

                base_agent
                    .conversation_history
                    .push(llm_response.choices[0].message.clone());

                let stop_reason =
                    should_stop_fn(&llm_response, current_step_number, base_agent.max_steps);

                match stop_reason {
                    StopReason::TaskCompleted => {
                        agent_step.state = AgentState::Completed;
                        execution.success = true;
                        execution.final_result = process_completion_fn(&llm_response);
                        info!(task = %task_name, "Task marked completed by agent logic.");
                        agent_step.duration_ms = step_start_time.elapsed().as_millis();
                        execution.steps.push(agent_step);
                        break; // Exit the main while loop
                    }
                    StopReason::ValidationFailed(validation_msg) => {
                        warn!(task = %task_name, "Task completion validation failed: {}", validation_msg);
                        base_agent.conversation_history.push(LLMMessage {
                            role: MessageRole::User,
                            content: Some(validation_msg),
                            name: None,
                            tool_calls: None,
                            tool_call_id: None,
                        });
                        agent_step.state = AgentState::Thinking; // Will continue to next LLM call
                    }
                    StopReason::MaxStepsReached => {
                        // This case should ideally be caught by the loop condition,
                        // but fn_should_stop also checks it. If it returns MaxStepsReached,
                        // we treat it as a form of non-successful completion.
                        // The main loop's condition `current_step_number <= base_agent.max_steps`
                        // will handle breaking. This specific match arm might not be strictly
                        // necessary if fn_should_stop only returns MaxStepsReached when current_step >= max_steps.
                        // However, if fn_should_stop has other reasons to declare max steps (e.g. token limits),
                        // then this is a clean way to stop. For now, assume it's primarily for loop termination.
                        // The outer loop will break, and finalization logic will set error_message.
                        // No specific action here other than letting the loop condition handle it.
                        // The current fn_should_stop returns MaxStepsReached if current_step_number >= max_steps
                        // So, if it's exactly max_steps, this iteration runs, fn_should_stop might return MaxStepsReached,
                        // then current_step_number increments, and loop terminates.
                        // If it's already > max_steps, loop wouldn't run.
                        // For clarity, if MaxStepsReached is determined here, we can break.
                        warn!(task = %task_name, "Max steps reached as per should_stop_fn.");
                        // Let the main loop condition handle the break and subsequent error message.
                        // If we break here, we need to ensure final_result/error_message is set appropriately.
                        // The existing logic outside the loop handles MaxStepsReached error message.
                        // So, we can simply let it flow, or break and ensure the message is set.
                        // To be safe and explicit:
                        execution.error_message = Some(AgentError::MaxStepsReached(base_agent.max_steps).to_string());
                        agent_step.state = AgentState::Failed; // Or a new "MaxStepsExceeded" state
                        agent_step.duration_ms = step_start_time.elapsed().as_millis();
                        execution.steps.push(agent_step);
                        break;
                    }
                    StopReason::Continue => {
                        // Continue with tool call checks or other logic for this step
                        let llm_message = &llm_response.choices[0].message;
                        if let Some(tool_calls) = llm_message.tool_calls.clone() {
                            if !tool_calls.is_empty() {
                                agent_step.state = AgentState::CallingTool;
                            agent_step.tool_calls_made = Some(tool_calls.clone());
                            if let Some(sender) = &event_sender {
                                _ = sender
                                    .send(AgentEvent::StepStateChange(
                                        current_step_number,
                                        AgentState::CallingTool,
                                    ))
                                    .await;
                                for tc_req in &tool_calls {
                                    _ = sender
                                        .send(AgentEvent::ToolCallAttempt(
                                            current_step_number,
                                            Box::new(tc_req.clone()),
                                        ))
                                        .await;
                                }
                            }

                            let executed_tool_results = if base_agent.config.get_current_provider_config().map_or(false, |pc| pc.parallel_tool_calls) {
                                debug!("Executing tool calls in parallel (mode)");
                                base_agent.tool_executor.parallel_tool_calls(&tool_calls).await
                            } else {
                                debug!("Executing tool calls sequentially (mode)");
                                base_agent.tool_executor.sequential_tool_calls(&tool_calls).await
                            };
                            agent_step.tool_results = Some(executed_tool_results.clone());
                            if let Some(sender) = &event_sender {
                                for tres in &executed_tool_results {
                                    _ = sender
                                        .send(AgentEvent::ToolCallResult(
                                            current_step_number,
                                            Box::new(tres.clone()),
                                        ))
                                        .await;
                                }
                            }

                            for tool_result in executed_tool_results {
                                let tool_name_for_message = tool_calls
                                    .iter()
                                    .find(|tc| tc.id == tool_result.tool_call_id)
                                    .map(|tc| tc.function.name.clone());
                                base_agent.conversation_history.push(LLMMessage {
                                    tool_call_id: Some(tool_result.tool_call_id),
                                    role: MessageRole::Tool,
                                    name: tool_name_for_message,
                                    content: Some(tool_result.result.unwrap_or_else(|| {
                                        tool_result.error.unwrap_or_else(|| {
                                            "Error executing tool, no details.".to_string()
                                        })
                                    })),
                                    tool_calls: None,
                                });
                            }
                            agent_step.state = AgentState::ProcessingToolResult;
                        } else {
                            // No tool calls, and StopReason was Continue.
                            // This is where Python sends "It seems that you have not completed the task."
                            debug!(
                                step = current_step_number,
                                "LLM provided text response without tool calls, and task is not yet complete. Re-prompting."
                            );
                            base_agent.conversation_history.push(LLMMessage {
                                role: MessageRole::User,
                                content: Some("It seems that you have not completed the task. Please try again or use a tool to signal completion.".to_string()),
                                name: None,
                                tool_calls: None,
                                tool_call_id: None,
                            });
                            // agent_step.state remains Thinking or whatever it was before this specific sub-branch
                            // if it was Thinking from the start of the match, it stays Thinking.
                        }
                    } else {
                        // No tool_calls field in the message, and StopReason was Continue.
                        debug!(
                            step = current_step_number,
                            "LLM provided text response without tool calls (tool_calls field was None), and task is not yet complete. Re-prompting."
                        );
                        base_agent.conversation_history.push(LLMMessage {
                            role: MessageRole::User,
                            content: Some("It seems that you have not completed the task. Please try again or use a tool to signal completion.".to_string()),
                            name: None,
                            tool_calls: None,
                            tool_call_id: None,
                        });
                    }
                } // Closes StopReason::Continue block
            } // Closes match stop_reason
        } // Closes Ok(llm_response) block
        Err(e) => { // This is the Err arm for llm_response_result
            error!(step = current_step_number, error = %e, "LLM chat failed");
            agent_step.state = AgentState::Failed;
                agent_step.error = Some(e.to_string());
                execution.error_message = Some(e.to_string());
                execution.success = false;
                agent_step.duration_ms = step_start_time.elapsed().as_millis();
                execution.steps.push(agent_step);
                break;
            }
        }
        agent_step.duration_ms = step_start_time.elapsed().as_millis();

        // Record the step with the trajectory recorder if available
        if let Some(recorder) = base_agent.trajectory_recorder.as_mut() {
            // The AgentStep needs all relevant information before being cloned and recorded.
            // Ensure messages_to_llm, llm_response, tool_calls_made, tool_results, reflection, error are populated.
            recorder.record_agent_step(agent_step.clone());
        }
        execution.steps.push(agent_step);
        current_step_number += 1;
    }

    // Finalize trajectory recording
    if let Some(recorder) = base_agent.trajectory_recorder.as_mut() {
        let _ = recorder.finalize_recording(
            execution.success,
            execution.final_result.clone(),
            // TODO: Properly aggregate LLMUsage if it's not directly on execution.
            execution.total_tokens_used.clone() // Pass the Option<LLMUsage> directly
        );
    }

    if !execution.success && execution.error_message.is_none() {
        warn!(task = %task_name, steps = current_step_number -1, "Task reached max steps or ended without explicit completion/failure.");
        if current_step_number > base_agent.max_steps {
            execution.error_message =
                Some(AgentError::MaxStepsReached(base_agent.max_steps).to_string());
        } else {
            execution.error_message =
                Some("Task ended without explicit completion signal.".to_string());
        }
    }

    execution.end_time = Some(
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
    );

    if let Some(sender) = event_sender {
        if execution.success {
            _ = sender
                .send(AgentEvent::TaskCompleted(Box::new(execution.clone())))
                .await;
        } else {
            _ = sender
                .send(AgentEvent::TaskFailed(Box::new(execution.clone())))
                .await;
        }
    }
    info!(task = %task_name, success = execution.success, "Task execution finished.");
    Ok(execution)
}
